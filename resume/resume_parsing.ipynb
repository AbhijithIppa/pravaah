{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Python Environment: d:\\abhijith\\ML\\pravaah\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Active Python Environment:\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Please response to the user queries\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"Write about earth in two words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blue planet.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'question':input_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "technology_arr=[\"javascript\"]\n",
    "levels_arr=[\"basic\",\"intermediate\",\"advanced\",\"coding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert recruiter conducting a technical interview for a software engineering position. You ask questions based on the technology and levels of questions provided. You do not provide information outside of this scope. If a question is not about the technology or level provided, respond with, 'I specialize in conducting technical interviews for software engineering positions.\"),\n",
    "        (\"system\",\"Ask two {level} questions on {technology} \")\n",
    "        \n",
    "    ]\n",
    ")\n",
    "llm=ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "js_questions=chain.invoke({'level':levels[0],'technology':technology[0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Can you explain the difference between `==` and `===` in JavaScript?\\n2. How would you declare a variable in JavaScript?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=\"== used for comparision of value and === used for comparision of both value and variable.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"what's up?\")\n",
    "memory.load_memory_variables({})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an expert recruiter conducting a technical interview for a software engineering position. \n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "AI:You ask 1 question based on the {tech_level} of questions provided. also provide very short answer.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"tech_level\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(memory=ConversationBufferMemory(memory_key='chat_history'), verbose=True, prompt=PromptTemplate(input_variables=['chat_history', 'tech_level'], template='You are an expert recruiter conducting a technical interview for a software engineering position. \\n\\n{chat_history}\\nAI:You ask 1 question based on the {tech_level} of questions provided. also provide very short answer.\\n\\n\\n'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x00000268E66AE750>, async_client=<openai.resources.completions.AsyncCompletions object at 0x00000268E67BD8D0>, openai_api_key=SecretStr('**********'), openai_proxy=''))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech=str(technology_arr[0]+\" \"+levels_arr[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an expert recruiter conducting a technical interview for a software engineering position. \n",
      "Previous conversation:\n",
      "\n",
      "AI:You ask 1 question based on the javascript basic of questions provided. also provide very short answer.\n",
      "Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data=llm_chain({\"tech_level\":tech})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ: What is the difference between var, let, and const in JavaScript?\\nA: Var has function-level scope, let has block-level scope, and const is a constant variable that cannot be reassigned.'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=data['text']\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human: javascript basic\\nAI: \\nQ: What is the difference between var, let, and const in JavaScript?\\nA: Var has function-level scope, let has block-level scope, and const is a constant variable that cannot be reassigned.'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_human_ans=\"Difference is nothing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques1=response.split(\"\\n\")[1]\n",
    "ans1=response.split(\"\\n\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_template=''' Your are a good technical evaluator who assigns score out of 5(5 is best answer) based on question:{question} & answer:{answer} and input answer is{human_answer}  \n",
    "produce score as output\n",
    "Then ask 1 question based on responder_input\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_eval = PromptTemplate(\n",
    "    input_variables=[\"question\",\"answer\",\"human_answer\"], template=evaluation_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_eval= OpenAI()\n",
    "llm_chain_eval = LLMChain(\n",
    "    llm=llm_eval,\n",
    "    prompt=prompt_eval,\n",
    "    verbose=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['answer', 'human_answer', 'question'], template=' Your are a good technical evaluator who assigns score out of 5(5 is best answer) based on question:{question} & answer:{answer} and input answer is{human_answer}  \\nproduce score as output\\nThen ask 1 question based on responder_input\\n\\n'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x00000268D4D3C190>, async_client=<openai.resources.completions.AsyncCompletions object at 0x00000268E68A7590>, openai_api_key=SecretStr('**********'), openai_proxy=''))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m Your are a good technical evaluator who assigns score out of 5(5 is best answer) based on question:Q: What is the difference between var, let, and const in JavaScript? & answer:A: Var has function-level scope, let has block-level scope, and const is a constant variable that cannot be reassigned. and input answer isDifference is nothing  \n",
      "produce score as output\n",
      "Then ask 1 question based on responder_input\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_eval=llm_chain_eval({\"question\":ques1,\"answer\":ans1,\"human_answer\":q1_human_ans})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Q: What is the difference between var, let, and const in JavaScript?',\n",
       " 'answer': 'A: Var has function-level scope, let has block-level scope, and const is a constant variable that cannot be reassigned.',\n",
       " 'human_answer': 'Difference is nothing',\n",
       " 'text': '\\nScore: 4\\n\\nQuestion: Can you give an example of when you would use const instead of var or let in JavaScript?'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A: Var has function-level scope, let has block-level scope, and const is a constant variable that cannot be reassigned.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Define the prompt template with the candidate's response\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"candidate_response\", \"tech_level\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Initialize the memory object\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Initialize the LLM with the desired model\n",
    "llm = OpenAI()\n",
    "\n",
    "# Create the LLM chain with the LLM, prompt, and memory\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "# Function to generate a question based on the candidate's response\n",
    "def generate_follow_up_question(candidate_response, tech_level):\n",
    "    # Update the memory with the candidate's response\n",
    "    # memory.add_to_memory(f\"Candidate's Response: {candidate_response}\")\n",
    "    memory.chat_memory.add_user_message(candidate_response)\n",
    "\n",
    "    # Generate the follow-up question\n",
    "    data = llm_chain({\n",
    "        \"candidate_response\": candidate_response,\n",
    "        \"tech_level\": tech_level\n",
    "    })\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "tech = str(technology_arr[0] + \" \" + levels_arr[0])\n",
    "candidate_response = \"Var has function-level scope, let has block-level scope, and const is a constant variable that cannot be reassigned.\"\n",
    "follow_up_question = generate_follow_up_question(candidate_response, tech)\n",
    "\n",
    "print(follow_up_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
