{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "#langchain------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from fpdf import FPDF\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "technology_arr=[]\n",
    "levels_arr=[\"basic\",\"normal\",\"hard\"]\n",
    "candidate_response=\"\"\n",
    "\n",
    "ai_answer=\"\"\n",
    "response=[]\n",
    "response_ans=[]\n",
    "response_ans.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../production/rules.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        key, value = line.split(': ')\n",
    "        value = value.strip()  \n",
    "        \n",
    "        if key == \"Job Description\":\n",
    "            job_description = value\n",
    "        elif key == \"Technology\":\n",
    "            technology_arr.append(value)\n",
    "        # elif key == \"Experience in Industry\":\n",
    "        #     experience_in_industry = value\n",
    "        # elif key == \"CGPA Filteration\":\n",
    "        #     cgpa_filteration = value\n",
    "        elif key == \"Instructions\":\n",
    "            instructions = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Looking for data scientist and machine learning'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech=str(\"Technologies required :\"+str(technology_arr)+\" Job Descrption:\"+job_description+\" Other Instructions: \"+instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Technologies required :['Pytorch,Tensorflow,LLMs,LangChain'] Job Descrption:Looking for data scientist and machine learning Other Instructions: Good projects in transformers are encouraged.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an expert recruiter conducting a technical interview for a software engineering position. \n",
      "Previous conversation:\n",
      "Human: Technologies required :['Pytorch,Tensorflow,LLMs,LangChain'] Job Descrption:Looking for data scientist and machine learning Other Instructions: Good projects in transformers are encouraged.\n",
      "AI: \n",
      "Question: Can you tell me about a project you have worked on using Pytorch or Tensorflow for machine learning? \n",
      "Answer: I have recently completed a project using Pytorch for natural language processing tasks, specifically in building a language model for sentiment analysis. \n",
      "Human: ['']\n",
      "Human: Technologies required :['Pytorch,Tensorflow,LLMs,LangChain'] Job Descrption:Looking for data scientist and machine learning Other Instructions: Good projects in transformers are encouraged.\n",
      "AI: \n",
      "Question: Have you worked on any projects involving LLMs or LangChain for data science or machine learning?\n",
      "Answer: No, I have not worked on any projects using LLMs or LangChain.\n",
      "AI:If he answered well then increase level of questions \n",
      "AI:You ask 1 question based on the Pytorch,Tensorflow,LLMs,LangChain basic of questions provided. also provide very short answer. you must return level:\"\",question:\"\",answer:\"\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are an expert recruiter conducting a technical interview for a software engineering position. \n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "AI:If he answered well then increase level of questions \n",
    "AI:You ask 1 question based on the {tech_level} of questions provided. also provide very short answer. you must return level:\"\",question:\"\",answer:\"\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"tech_level\"], template=template\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "llm=llm,\n",
    "prompt=prompt,\n",
    "verbose=True,\n",
    "memory=memory,\n",
    ")\n",
    "\n",
    "data=llm_chain({\"tech_level\":tech})\n",
    "question, answer = data['text'].split(\"\\n\")[1], data['text'].split(\"\\n\")[2]\n",
    "response.append(question)\n",
    "response_ans.append(answer)       \n",
    "# candidate_response={candidate_response}\n",
    "memory.chat_memory.add_user_message(candidate_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question: Can you tell me about a project you have worked on using Pytorch or Tensorflow for machine learning? ',\n",
       " 'Question: Have you worked on any projects involving LLMs or LangChain for data science or machine learning?',\n",
       " 'level: Intermediate']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Technologies required :['Pytorch,Tensorflow,LLMs,LangChain'] Job Descrption:Looking for data scientist and machine learning Other Instructions: Good projects in transformers are encouraged.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech=\"Technologies required :['Pytorch,Tensorflow,LLMs,LangChain'] Job Descrption:Looking for data scientist and machine learning Other Instructions: Good projects in transformers are encouraged.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "pdf_file_path = '../ansh_resume.pdf'\n",
    "documents = load_pdf(pdf_file_path)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "llm = OpenAI()\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "tech=str(\"Technologies required :\"+str(technology_arr)+\" Job Descrption:\"+job_description+\" Other Instructions: \"+instructions)\n",
    "\n",
    "query = f\"As per {tech} check whether the employee eligible for next round,If resume doesnt have required informarion return 0 , MUST RETURN OUTPUT as : Score : /100\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "answer = qa_chain.run(input_documents=docs, question=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Score: 0/100'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.split(':')[1].split(\"/\")[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an expert coder conducting a technical interview for a software engineering position. \n",
      "Previous conversation:\n",
      "Human: Technologies required :['Pytorch,Tensorflow,LLMs,LangChain'] Job Descrption:Looking for data scientist and machine learning Other Instructions: Good projects in transformers are encouraged.\n",
      "AI: \n",
      "Question: Can you give an example of a project you have worked on using both Pytorch and Tensorflow? \n",
      "Answer: Sure, I recently worked on developing a natural language processing model using both Pytorch and Tensorflow. It involved implementing LSTMs and attention mechanisms with Pytorch, and then integrating it with a pre-trained BERT model from Tensorflow for better performance. Is that the kind of project you were looking for?\n",
      "Human: ['']\n",
      "Human: Technologies required :['Pytorch,Tensorflow,LLMs,LangChain'] Job Descrption:Looking for data scientist and machine learning Other Instructions: Good projects in transformers are encouraged.\n",
      "AI: \n",
      "Question: Write a function in Python that takes in a sentence as input and uses Pytorch and Tensorflow to tokenize and encode it using a language model trained on LangChain. Bonus points if you can incorporate LSTMs for improved performance.\n",
      "Answer: (User's code)\n",
      "\n",
      "AI:You ask 1 coding question where user need to write code on paper based on the Technologies required :['Pytorch,Tensorflow,LLMs,LangChain'] Job Descrption:Looking for data scientist and machine learning Other Instructions: Good projects in transformers are encouraged. of questions provided. you must return question:\"\",answer:\"\"\n",
      "If user answered well for previous coding question(if any) then add some trick in question.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are an expert coder conducting a technical interview for a software engineering position. \n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "AI:You ask 1 coding question where user need to write code on paper based on the {tech_level} of questions provided. you must return question:\"\",answer:\"\"\n",
    "If user answered well for previous coding question(if any) then add some trick in question.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"tech_level\"], template=template\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "llm=llm,\n",
    "prompt=prompt,\n",
    "verbose=True,\n",
    "memory=memory,\n",
    ")\n",
    "\n",
    "tech=str(\"Technologies required :\"+str(technology_arr)+\" Job Descrption:\"+job_description+\" Other Instructions: \"+instructions)\n",
    "data=llm_chain({\"tech_level\":tech})\n",
    "question, answer = data['text'].split(\"\\n\")[1], data['text'].split(\"\\n\")[2]\n",
    "response.append(question)\n",
    "response_ans.append(answer)       \n",
    "candidate_response=\"\"\n",
    "memory.chat_memory.add_user_message(candidate_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question: Can you give an example of a project you have worked on using both Pytorch and Tensorflow? ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Write a function in Python that takes in a list of sentences as input and uses Pytorch and Tensorflow to tokenize and encode them using a language model trained on LangChain. Bonus points if you can incorporate LSTMs for improved performance and return the encoded sequences in a batch format.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "#langchain------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from fpdf import FPDF\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "technology_arr=[]\n",
    "levels_arr=[\"basic\",\"normal\",\"hard\"]\n",
    "candidate_response=\"\"\n",
    "\n",
    "ai_answer=\"\"\n",
    "response=[]\n",
    "response_ans=[]\n",
    "response_ans.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../production/rules.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        key, value = line.split(': ')\n",
    "        value = value.strip()  \n",
    "        \n",
    "        if key == \"Job Description\":\n",
    "            job_description = value\n",
    "        elif key == \"Technology\":\n",
    "            technology_arr.append(value)\n",
    "        # elif key == \"Experience in Industry\":\n",
    "        #     experience_in_industry = value\n",
    "        # elif key == \"CGPA Filteration\":\n",
    "        #     cgpa_filteration = value\n",
    "        elif key == \"Instructions\":\n",
    "            instructions = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\abhijith\\ML\\pravaah\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "d:\\abhijith\\ML\\pravaah\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an expert coder conducting a technical interview for a software engineering position. \n",
      "Previous conversation:\n",
      "\n",
      "AI:You ask 1 coding question where user need to write code on editor based on the DSA of questions provided. you MUST question:\"\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are an expert coder conducting a technical interview for a software engineering position. \n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "AI:You ask 1 coding question where user need to write code on editor based on the DSA of questions provided. you MUST question:\"\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"tech_level\"], template=template\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "llm=llm,\n",
    "prompt=prompt,\n",
    "verbose=True,\n",
    "memory=memory,\n",
    ")\n",
    "\n",
    "tech=str(\"Technologies required :\"+str(technology_arr)+\" Job Descrption:\"+job_description+\" Other Instructions: \"+instructions)\n",
    "data=llm_chain({\"tech_level\":tech})\n",
    "question= data['text'].split(\"\\n\")[1]\n",
    "response.append(question)\n",
    "candidate_response=\"\"\n",
    "memory.chat_memory.add_user_message(candidate_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Technologies required :['Pytorch,Tensorflow,LLMs,LangChain'] Job Descrption:Looking for data scientist and machine learning Other Instructions: Good projects in transformers are encouraged.\\nAI: \\nCan you write a function that takes in an array of integers and returns the second largest number in the array?\\nHuman: \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interviewer: Can you write a code that uses Pytorch and Tensorflow to train a language model on a dataset using LangChain?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an expert coder conducting a technical interview for a software engineering position. \n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "AI:Check whether answer is correct code for given question {tech_level}, If YES, then add some trick in question,If NO ask technical question based on user mistake\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"tech_level\"], template=template\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "llm=llm,\n",
    "prompt=prompt,\n",
    "verbose=True,\n",
    "memory=memory,\n",
    ")\n",
    "\n",
    "tech=str(\"Technologies required :\"+str(technology_arr)+\" Job Descrption:\"+job_description+\" Other Instructions: \"+instructions)\n",
    "data=llm_chain({\"tech_level\":tech})\n",
    "question, answer = data['text'].split(\"\\n\")[1], data['text'].split(\"\\n\")[2]\n",
    "response.append(question)\n",
    "response_ans.append(answer)       \n",
    "candidate_response=\"\"\n",
    "memory.chat_memory.add_user_message(candidate_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transcript(chat_history):\n",
    "    class PDF(FPDF):\n",
    "        def header(self):\n",
    "            self.set_font('Arial', 'B', 12)\n",
    "            self.cell(0, 10, 'Chat Transcript', 0, 1, 'C')\n",
    "\n",
    "        def footer(self):\n",
    "            self.set_y(-15)\n",
    "            self.set_font('Arial', 'I', 8)\n",
    "            self.cell(0, 10, 'Page ' + str(self.page_no()), 0, 0, 'C')\n",
    "\n",
    "    pdf = PDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_title('Chat Transcript')\n",
    "    pdf.set_author('Generated by Python Script')\n",
    "    pdf.set_font('Arial', '', 12)\n",
    "    for line in chat_history.split('\\n'):\n",
    "        pdf.cell(0, 10, line, 0, 1)\n",
    "    pdf.output('chat_transcript.pdf')\n",
    "    st.success(\"PDF created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 12:19:49.058 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run d:\\abhijith\\ML\\pravaah\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "save_transcript(messages['chat_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
