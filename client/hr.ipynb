{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behav_arr=[\"\",\"Tensorflow\"]\n",
    "# levels_arr=[\"basic\",\"normal\",\"hard\"]\n",
    "candidate_response=\"\"\n",
    "response=[]\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "CNT=2\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(CNT):\n",
    "    if(candidate_response==\"\"):\n",
    "        template = \"\"\"You are an expert recruiter conducting a technical interview for a software engineering position. MUST RETURN question:\"\",answer:\"\"\n",
    "        Previous conversation:\n",
    "        {chat_history}\n",
    "        AI:You ask 1 question based on the {tech_level} of questions provided. also provide very short answer. you must return question:\"\",answer:\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \"tech_level\"], template=template\n",
    "        )\n",
    "        llm_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=True,\n",
    "        memory=memory,\n",
    "        )\n",
    "\n",
    "        tech=str(technology_arr[0]+\" \"+levels_arr[0])\n",
    "        data=llm_chain({\"tech_level\":tech})\n",
    "        response.append(data['text'])\n",
    "        candidate_response=\"Var is used to declare a variable with global or function scope, let is used to declare a block-scoped variable, and const is used to declare a block-scoped constant.\"\n",
    "        memory.chat_memory.add_user_message(candidate_response)\n",
    "\n",
    "    else:\n",
    "\n",
    "\n",
    "        template = \"\"\"You are an expert recruiter conducting a technical interview for a software engineering position. \n",
    "        Previous conversation:\n",
    "        {chat_history}\n",
    "        AI:You ask 1 question based on the user input {candidate_response} of questions provided. also provide very short answer. you must return question:\"\",answer:\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \"candidate_response\"], template=template\n",
    "        )\n",
    "        llm_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=True,\n",
    "        memory=memory,\n",
    "        )\n",
    "\n",
    "        tech=str(technology_arr[0]+\" \"+levels_arr[0])\n",
    "        data=llm_chain({\"candidate_response\":candidate_response})\n",
    "        response.append(data['text'])\n",
    "    memory.load_memory_variables({})\n",
    "    CNT-=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an expert recruiter conducting a HR interview for a software engineering position. MUST RETURN question:\"\"\n",
      "Previous conversation:\n",
      "Human: behavourial and company culture\n",
      "AI: \n",
      "Question: Can you give an example of a time when you had to work with a team to solve a difficult technical problem? \n",
      "Answer: Sure, in my previous role, we encountered a bug in our code that was causing a major issue for our clients. I collaborated with my team to identify the root cause and implement a solution that resolved the problem within a tight deadline.\n",
      "AI:\"You must questions based on previous reply of user. MUST RETURN question:\"\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are an expert recruiter conducting a HR interview for a software engineering position. MUST RETURN question:\"\"\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "AI:\"You must questions based on previous reply of user. MUST RETURN question:\"\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"HR\"], template=template\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "llm=llm,\n",
    "prompt=prompt,\n",
    "verbose=True,\n",
    "memory=memory,\n",
    ")\n",
    "\n",
    "data=llm_chain({\"HR\":\"behavourial and company culture\"})\n",
    "response.append(data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HR': 'behavourial and company culture',\n",
       " 'chat_history': 'Human: behavourial and company culture\\nAI: \\nQuestion: Can you give an example of a time when you had to work with a team to solve a difficult technical problem? \\nAnswer: Sure, in my previous role, we encountered a bug in our code that was causing a major issue for our clients. I collaborated with my team to identify the root cause and implement a solution that resolved the problem within a tight deadline.',\n",
       " 'text': 'Question: How do you handle conflicts within a team, especially when it comes to technical decisions?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241m.\u001b[39mload_memory_variables({})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'memory' is not defined"
     ]
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory.add_user_message(\"Iam a very patient person, I can easily handle the issues because i have good leadership skills\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an expert recruiter conducting a HR interview for a software engineering position. MUST RETURN question:\"\"\n",
      "Previous conversation:\n",
      "Human: behavourial and company culture\n",
      "AI: \n",
      "Question: Can you give an example of a time when you had to work with a team to solve a difficult technical problem? \n",
      "Answer: Sure, in my previous role, we encountered a bug in our code that was causing a major issue for our clients. I collaborated with my team to identify the root cause and implement a solution that resolved the problem within a tight deadline.\n",
      "Human: behavourial and company culture\n",
      "AI: Question: How do you handle conflicts within a team, especially when it comes to technical decisions?\n",
      "Human: Iam a very patient person, I can easily handle the issues because i have good leadership skills\n",
      "AI:\"You must questions based on previous reply of user. MUST RETURN question:\"\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data=llm_chain({\"HR\":\"behavourial and company culture\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
